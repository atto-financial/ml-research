cv_roc_auc,cv_accuracy,cv_precision,cv_recall,cv_f1,cv_score_mean,cv_score_std,test_roc_auc,test_accuracy,test_precision,test_recall,test_f1,accuracy_ci_lower,accuracy_ci_upper,precision_ci_lower,precision_ci_upper,recall_ci_lower,recall_ci_upper,f1_ci_lower,f1_ci_upper,roc_auc_ci_lower,roc_auc_ci_upper,overfitting_gap,cost_loss,final_features,model_path,model_checksum,scaler_path,scaler_checksum,metadata_path,metadata_checksum,timestamp,decision_threshold,scoring,n_estimators,max_depth,min_samples_split,n_features_to_select,n_folds,random_state,oversampling_method,categorical_features,use_undersampling,max_vif,n_bootstraps,alpha,test_size,memory_threshold,cost_weight,package_versions
0.918639053254438,0.9038461538461539,0.8888888888888888,0.9230769230769231,0.9056603773584906,0.9199999999999999,0.09797958971132709,0.7083333333333334,0.75,0.5,0.5,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15384615384615385,0.0,"[{""feature"": ""loan_extravagance_interaction"", ""importance"": 0.5645919650742974}, {""feature"": ""worship_to_vigilance_ratio"", ""importance"": 0.4354080349257027}]",save_models/custom_model_20250716_163822.pkl,d37a16d59c8f43df3b3bd79110396bbfc465080d10c440bd279f9f241f69f72e,save_scalers/custom_scaler_20250716_163822.pkl,6735757a50b397a5b821414244e256cde9bbdc95bb06e20ba48d29ba27d20df4,,,20250716_163822,0.5,f1,"[50, 100, 200]","[3, 5, 10, null]","[2, 5, 10]",10,5,42,borderline,[],False,1.5,500,0.05,0.2,1000000000,"{""0"": 1, ""1"": 1}","{""sklearn"": ""1.6.1"", ""joblib"": ""1.3.2"", ""pandas"": ""1.5.3"", ""numpy"": ""1.26.4""}"
